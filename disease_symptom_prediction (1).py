# -*- coding: utf-8 -*-
"""Disease Symptom Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-U3kvC5FmV7M7RG_u6gGF389yK2MP5t

# Disease Symptom Prediction

# Introduction

This project aims to build a machine learning model capable of predicting possible diseases based on a set of symptoms provided by the user. By analyzing the relationship between symptoms and their corresponding diagnoses, the model assists in early detection and decision making for medical professionals and patients.
The approach involves data preprocessing, handling missing values, feature encoding, and applying classification algorithms such as Random Forest to achieve accurate predictions. Special attention is given to model evaluation using metrics like accuracy, precision, recall, and F1-score to ensure reliable results.

Data Source: https://www.kaggle.com/datasets/itachi9604/disease-symptom-description-dataset/data

**Amar Ahmed**

**Data Scientist** | **Machine Learning Engineer**  | **Computer Vision Researcher**

# Installing
"""

!pip install ydata_profiling # EDA

!pip install gradio

"""# Load Data & Import Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import gradio as gr
import joblib
from ydata_profiling import ProfileReport
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from wordcloud import WordCloud


# Read Data
Data = pd.read_csv("dataset.csv")
Data

# Data Overview
display("Data size:", Data.shape)
print('--------------------------------------------')
display("\n columns:", Data.columns.tolist())
print('--------------------------------------------')
display("\n Data type:", Data.dtypes)
print('--------------------------------------------')

Data.info()

Data.describe().T

display(Data.isnull().sum())

plt.figure(figsize=(11, 5))
sns.heatmap(Data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()
plt.savefig('Missing_Heatmap DSP.png')

Data.duplicated().sum()

Data.sample(10)

Data.columns

Data['Disease'].value_counts()

Data['Symptom_1'].value_counts()

Data['Symptom_2'].value_counts()

Data[Data['Disease'] == Data['Disease'].max()]['Symptom_1']

Data[Data['Disease'] == Data['Disease'].min()]['Symptom_1']

"""# **Analysis**

"""

plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Symptom_15'].value_counts()
    for x_label, grp in Data.groupby('Symptom_14')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('Symptom_14')
_ = plt.ylabel('Symptom_15')

Data.groupby('Symptom_14').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['Symptom_17'].value_counts()
    for x_label, grp in Data.groupby('Symptom_16')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('Symptom_16')
_ = plt.ylabel('Symptom_17')

"""# Full Report


"""

# ProfileReport(Data, title="Profiling Report")

"""# Data Cleaning & Preprocessing"""

def preprocess_data(df):
    """Preprocess the dataset for modeling"""

    print("\n‚öôÔ∏è Preprocessing data...")

    # Handle missing values (if any)
    if df.isnull().sum().sum() > 0:
        print("Handling missing values...")
        # For symptoms, we'll treat missing as absence of symptom
        symptom_cols = [col for col in df.columns if col.startswith('Symptom_')]
        df[symptom_cols] = df[symptom_cols].fillna('No Symptom')

    # Encode symptoms into binary features
    print("Encoding symptoms...")
    all_symptoms = set()
    for col in df.columns[1:18]:  # Include all Symptom columns
        all_symptoms.update(df[col].unique())

    all_symptoms.discard('No Symptom')

    # Convert the set to a list of strings before sorting
    all_symptoms = sorted([str(s) for s in list(all_symptoms)])


    # Create binary features for each symptom
    for symptom in all_symptoms:
        df[symptom] = 0

    for idx, row in df.iterrows():
        for col in df.columns[1:18]:  # Symptom columns
            symptom = row[col]
            if symptom != 'No Symptom':
                df.at[idx, symptom] = 1

    # Prepare X and y
    X = df[all_symptoms]
    y = df['Disease']

    # Encode target variable
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Save encoders for later use
    joblib.dump(le, 'label_encoder.pkl')
    joblib.dump(all_symptoms, 'symptom_list.pkl')

    print(f"‚úÖ Data preprocessed. Final shape: {X.shape}")
    print(f"Number of unique diseases: {len(le.classes_)}")

    return X, y_encoded, le, all_symptoms

X, y, le, symptom_list = preprocess_data(Data)

"""# **Train Model**"""

def train_model(X, y):
    """Train and tune machine learning model"""

    print("\nüèãÔ∏è Training model...")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    # Define models and parameters for tuning
    models = {
        'RandomForest': {
            'model': RandomForestClassifier(random_state=42),
            'params': {
                'n_estimators': [100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5]
            }
        },
        'XGBoost': {
            'model': XGBClassifier(random_state=42, eval_metric='mlogloss'),
            'params': {
                'n_estimators': [100, 200],
                'max_depth': [3, 6],
                'learning_rate': [0.01, 0.1]
            }
        }
    }

    best_model = None
    best_score = 0
    best_model_name = ""

    # Perform grid search for each model
    for name, model_info in models.items():
        print(f"\nTuning {name}...")

        gs = GridSearchCV(
            estimator=model_info['model'],
            param_grid=model_info['params'],
            cv=5,
            scoring='accuracy',
            n_jobs=-1,
            verbose=1
        )

        gs.fit(X_train, y_train)

        # Evaluate on test set
        test_score = gs.score(X_test, y_test)

        print(f"{name} best params: {gs.best_params_}")
        print(f"{name} test accuracy: {test_score:.4f}")

        if test_score > best_score:
            best_score = test_score
            best_model = gs.best_estimator_
            best_model_name = name

    print(f"\nüèÜ Best model: {best_model_name} with accuracy: {best_score:.4f}")

     # Save the best model
    joblib.dump(best_model, 'disease_predictor.pkl')

    # Generate classification report
    y_pred = best_model.predict(X_test)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Confusion matrix
    plt.figure(figsize=(15, 12))
    sns.heatmap(confusion_matrix(y_test, y_pred),
                annot=True, fmt='d',
                xticklabels=le.classes_,
                yticklabels=le.classes_)
    plt.title('Confusion Matrix')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.close()

    return best_model

model = train_model(X, y)

model = joblib.load('disease_predictor.pkl')
le = joblib.load('label_encoder.pkl')
symptoms = joblib.load('symptom_list.pkl')

"""# **Building Application For Gradio**"""

def predict_disease(*selected_symptoms):
    """Predict disease based on selected symptoms"""

    # Load model and encoders
    try:
        model = joblib.load('disease_predictor.pkl')
        le = joblib.load('label_encoder.pkl')
    except:
        return "Error: Model not found. Please train the model first."

    # Create feature vector
    features = np.zeros(len(symptom_list))
    for i, symptom in enumerate(symptom_list):
        if selected_symptoms[i]:
            features[i] = 1

    # Make prediction
    try:
        prediction = model.predict([features])
        disease = le.inverse_transform(prediction)[0]

        # Get probability distribution
        probas = model.predict_proba([features])[0]
        top_3 = sorted(zip(le.classes_, probas),
                      key=lambda x: x[1], reverse=True)[:3]

        # Format output
        result = f"üîç Predicted Disease: {disease}\n\n"
        result += "Top 3 Probabilities:\n"
        for disease, prob in top_3:
            result += f"- {disease}: {prob*100:.2f}%\n"

        return result
    except Exception as e:
        return f"Error during prediction: {str(e)}"

# Create Gradio interface
print("\nüöÄ Creating Gradio interface...")

# Group symptoms into categories for better UI organization
symptom_groups = {
    'General': [s for s in symptom_list if 'fever' in s.lower() or 'fatigue' in s.lower()],
    'Respiratory': [s for s in symptom_list if 'cough' in s.lower() or 'breath' in s.lower()],
    'Digestive': [s for s in symptom_list if 'stomach' in s.lower() or 'vomit' in s.lower()],
    'Neurological': [s for s in symptom_list if 'headache' in s.lower() or 'dizziness' in s.lower()],
}

# Define the 'Other' group after symptom_groups is partially defined
symptom_groups['Other'] = [s for s in symptom_list if s not in [item for sublist in symptom_groups.values() for item in sublist]]


inputs = []
with gr.Blocks(title="Disease Predictor") as demo:
    gr.Markdown("# ü©∫ Disease Symptom Predictor")
    gr.Markdown("Select the symptoms you're experiencing to predict possible diseases.")

    # Create checkboxes organized by symptom groups
    selected_symptoms = []
    for group, symptoms in symptom_groups.items():
        if symptoms:  # Only add group if it has symptoms
            with gr.Group():
                gr.Markdown(f"### {group} Symptoms")
                for symptom in symptoms:
                    cb = gr.Checkbox(label=symptom)
                    selected_symptoms.append(cb)

    submit_btn = gr.Button("Predict Disease")
    output = gr.Textbox(label="Prediction Result")

    submit_btn.click(
        fn=predict_disease,
        inputs=selected_symptoms,
        outputs=output
    )

# Launch the interface
print("‚úÖ Interface created. Launching...")
demo.launch(share=True)

"""# **Thank you for checking out this notebook! ‚ù§Ô∏è‚ù§Ô∏è**"""